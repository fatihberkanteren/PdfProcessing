{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-27T21:52:37.000517Z",
     "iopub.status.busy": "2025-03-27T21:52:37.000085Z",
     "iopub.status.idle": "2025-03-27T21:52:48.799003Z",
     "shell.execute_reply": "2025-03-27T21:52:48.798168Z",
     "shell.execute_reply.started": "2025-03-27T21:52:37.000482Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install gradio pymupdf pillow torch transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:53:00.656842Z",
     "iopub.status.busy": "2025-03-27T21:53:00.656419Z",
     "iopub.status.idle": "2025-03-27T21:55:53.520946Z",
     "shell.execute_reply": "2025-03-27T21:55:53.519961Z",
     "shell.execute_reply.started": "2025-03-27T21:53:00.656813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "import hashlib\n",
    "import json\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # Flan-T5 for Q&A\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # Qwen for workflow\n",
    "from huggingface_hub import login\n",
    "\n",
    "# === Login and Device Setup ===\n",
    "login(token=\"HF_TOKEN\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Load Models ===\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", use_fast=True)\n",
    "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "qwen_model_id = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_model_id, use_fast=True)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# === Global State ===\n",
    "global_doc = []\n",
    "unique_images = {}\n",
    "\n",
    "# === PDF Content Extraction ===\n",
    "def extract_page_content(doc, page_number, seen_hashes):\n",
    "    page = doc[page_number]\n",
    "    text = page.get_text()\n",
    "    images = []\n",
    "    for img in page.get_images(full=True):\n",
    "        xref = img[0]\n",
    "        base_image = doc.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        image_hash = hashlib.md5(image_bytes).hexdigest()\n",
    "        if image_hash in seen_hashes:\n",
    "            continue\n",
    "        seen_hashes.add(image_hash)\n",
    "        image_pil = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        images.append(image_pil)\n",
    "    return text, images\n",
    "\n",
    "# === Image Captioning ===\n",
    "def describe_images(image_list):\n",
    "    if not image_list:\n",
    "        return []\n",
    "    inputs = processor(images=image_list, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = blip_model.generate(**inputs, max_new_tokens=50)\n",
    "        captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return [cap.strip() for cap in captions]\n",
    "\n",
    "# === Qwen: Workflow Generation ===\n",
    "def generate_workflow_qwen(full_text, full_captions):\n",
    "    prompt = f\"\"\"You are a technical documentation expert. Convert the following technical PDF content into a step-by-step workflow.\n",
    "Write only numbered steps using concise and technical language.\n",
    "\n",
    "Text:\n",
    "{full_text[:3000]}\n",
    "\n",
    "Image captions:\n",
    "{full_captions[:1000]}\n",
    "\n",
    "Workflow:\"\"\"\n",
    "    inputs = qwen_tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = qwen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    return qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# === Qwen: Interactive Guide Generation ===\n",
    "def generate_interactive_guide_qwen(full_text, full_captions):\n",
    "    prompt = f\"\"\"You are a technical documentation expert. Read the following technical PDF content and generate an interactive guide in JSON format.\n",
    "Return a valid JSON array where each element is an object with the following keys:\n",
    "  - \\\"name\\\": (string) The title of the step.\n",
    "  - \\\"type\\\": (string) One of \\\"instruction\\\", \\\"input_checkbox\\\", \\\"input_radio\\\", or \\\"input_number\\\".\n",
    "  - \\\"content\\\": (string) The main text or question for that step.\n",
    "  - \\\"options\\\": (optional, array) A list of strings for options if the type is \\\"input_checkbox\\\" or \\\"input_radio\\\".\n",
    "\n",
    "Text:\n",
    "{full_text[:3000]}\n",
    "\n",
    "Image captions:\n",
    "{full_captions[:1000]}\n",
    "\"\"\"\n",
    "    inputs = qwen_tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = qwen_model.generate(**inputs, max_new_tokens=1000, do_sample=False)\n",
    "    generated_text = qwen_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    try:\n",
    "        return json.dumps(json.loads(generated_text), indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"JSON parse error: {str(e)}\\nRaw output:\\n{generated_text}\"\n",
    "\n",
    "# === Qwen: Question Answering ===\n",
    "def generate_answer_with_qwen(question, context):\n",
    "    prompt = f\"\"\"You are a helpful assistant. Based ONLY on the context below, answer the user's question clearly and accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    inputs = qwen_tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = qwen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=400,\n",
    "            do_sample=False\n",
    "        )\n",
    "    output_text = qwen_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    # Extract only the answer part (after 'Answer:')\n",
    "    answer_start = output_text.find(\"Answer:\")\n",
    "    if answer_start != -1:\n",
    "        return output_text[answer_start + len(\"Answer:\"):].strip()\n",
    "    return output_text\n",
    "\n",
    "# === Semantic Search + Answer ===\n",
    "def answer_question(question):\n",
    "    if not global_doc:\n",
    "        return \"Lütfen önce bir PDF dosyası yükleyin.\"\n",
    "    chunks = [page[\"text\"] for page in global_doc]\n",
    "    for page in global_doc:\n",
    "        chunks.extend(page[\"image_descriptions\"])\n",
    "    q_emb = embedder.encode(question, convert_to_tensor=True)\n",
    "    doc_embs = embedder.encode(chunks, convert_to_tensor=True)\n",
    "    sims = util.cos_sim(q_emb, doc_embs)[0]\n",
    "    top_k = torch.topk(sims, k=min(5, len(sims)))\n",
    "    top_context = \"\\n\".join([chunks[i] for i in top_k.indices])\n",
    "    return generate_answer_with_qwen(question, top_context)\n",
    "\n",
    "# === Process PDF ===\n",
    "def process_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    global global_doc, unique_images\n",
    "    global_doc = []\n",
    "    unique_images = {}\n",
    "    full_text = \"\"\n",
    "    full_captions = \"\"\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        seen_hashes = set()\n",
    "        text, images = extract_page_content(doc, page_num, seen_hashes)\n",
    "        full_text += f\"\\n\\n--- Page {page_num+1} ---\\n\\n{text}\"\n",
    "        captions = describe_images(images) if images else []\n",
    "        full_captions += f\"\\n\\n--- Page {page_num+1} ---\\n\\n\" + \"\\n\".join([f\"Page {page_num+1} Image {i+1}: {cap}\" for i, cap in enumerate(captions)])\n",
    "        global_doc.append({\"page\": page_num + 1, \"text\": text, \"image_descriptions\": captions})\n",
    "        for img in images:\n",
    "            img_byte_arr = io.BytesIO()\n",
    "            img.save(img_byte_arr, format='PNG')\n",
    "            image_bytes = img_byte_arr.getvalue()\n",
    "            image_hash = hashlib.md5(image_bytes).hexdigest()\n",
    "            unique_images[image_hash] = img\n",
    "    workflow_markdown = generate_workflow_qwen(full_text, full_captions)\n",
    "    interactive_guide_json = generate_interactive_guide_qwen(full_text, full_captions)\n",
    "    return full_text, list(unique_images.values()), full_captions, workflow_markdown, interactive_guide_json\n",
    "\n",
    "# === Gradio UI ===\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 📄 PDF Analyzer: Text + Images + Workflow + Interactive Guide + Q&A\")\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "        process_btn = gr.Button(\"Process PDF\")\n",
    "    text_output = gr.Textbox(label=\"PDF Text\", lines=15)\n",
    "    image_output = gr.Gallery(label=\"Images\", show_label=False)\n",
    "    caption_output = gr.Textbox(label=\"Image Captions\", lines=10)\n",
    "    workflow_output = gr.Markdown(label=\"📋 Workflow\")\n",
    "    interactive_guide_output = gr.Textbox(label=\"Interactive Guide (JSON)\", lines=15)\n",
    "    gr.Markdown(\"## ❓ Ask a Question\")\n",
    "    with gr.Row():\n",
    "        q_in = gr.Textbox(label=\"Your Question\")\n",
    "        q_out = gr.Textbox(label=\"Answer\", lines=8)\n",
    "    q_in.submit(answer_question, inputs=q_in, outputs=q_out)\n",
    "    process_btn.click(\n",
    "        fn=process_pdf,\n",
    "        inputs=pdf_input,\n",
    "        outputs=[text_output, image_output, caption_output, workflow_output, interactive_guide_output]\n",
    "    )\n",
    "print(\"✅ Gradio app is running...\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
